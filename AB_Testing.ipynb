{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AB Testing Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general methodology used to test out a new product or feature\n",
    "\n",
    "- Take two sets of users\n",
    "- One set is shown an existing product\n",
    "- Second set is given a treated version\n",
    "- How do the customers respond differently? determine which one is better based on some metric\n",
    "\n",
    "Can you use AB tests for everything?\n",
    "\n",
    "\"AB testing is good for optimizing an existing product but not good for developing a new product based on an existing one\"\n",
    "\n",
    "Amazon did AB tests for personalized recommendations and found that they had an increase in revenue when given the personalized recommendations.\n",
    "\n",
    "Google tested 41 different shades of blue\n",
    "\n",
    "LinkedIn tested a ranking process where they checked whether it's better to show news articles or an encouragement to add more contacts on a users \"stream\". (Use click-through rate as metric?)\n",
    "\n",
    "Amazon determined that ever 100ms increase in page load time decreased sales by 1%\n",
    "\n",
    "Need a consistent response from your control and experiment groups\n",
    "\n",
    "What can't you do with AB tests?\n",
    "\n",
    "- **Test out new experiences**\n",
    "    - Change aversion - Users refuse to participate in the test\n",
    "    - Novelty effect - Too drastic of a change leads customers to \"try out everything\". Can't test out a specific treatment\n",
    "- No baseline for comparison\n",
    "    - Can't set up a control group if there is no baseline. \n",
    "- How much time do you need to have your users adapt to the new experience?\n",
    "    - Need the plateaued experience to make a \"robust decision\". The metric being observed will be noisy in the beginning and only when it has stabilised can you check for any statistically significant changes to your metric.\n",
    "- Long term effects are difficult to test\n",
    "    - Difficult to measure changes in your metric over a long time period where other aspects of your product or users will change (can't attribute the change in your metric to the treatment)\n",
    "- Can't test whether your missing something in your product\n",
    "    - No baseline, can;t set up a control and treatment group because what do you change about the treatment group?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other techniques**\n",
    "\n",
    "- Logs of what users did on your website. Analyse them retrospectively or observationally to see if a hypothesis can be developed about what caused changes in their behaviour. This can then be used to design an experiment. \n",
    "- User experience research, focus groups, surveys, human evaluation\n",
    "- A/B testing gives quantitative data, other techniques give qualitative data.\n",
    "- A completetly new product is difficult test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In online A/B tests, you don't know much about your users. You're using online user data and so it's difficult to distinguish whether a user is a single person, internet cafe etc.\n",
    "\n",
    "The goal is to determine whether a new feature is desirable. To do this, you need to design an experiment that can be **repeatable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Case Study\n",
    "\n",
    "Audacity\n",
    "\n",
    "Creates online finance courses\n",
    "\n",
    "**User flow/Customer funnel**\n",
    "\n",
    "- Homepage visits\n",
    "- Explore the site\n",
    "- Create an account\n",
    "- Completion\n",
    "\n",
    "Listed in decreasing number of users.\n",
    "\n",
    "**The hypothesis:**\n",
    "\n",
    "Changing the \"Start Now\" button from orange to pink will **increase** how many students explore Audacity's courses\n",
    "\n",
    "**Possible metrics**\n",
    "\n",
    "- ~~**Total number of courses completed**~~\n",
    "    - Will take too much time. Students make take months to complete a course\n",
    "- ~~**How many users click on the \"Start Now\" button**~~\n",
    "    - Assumes that users who progress through the top of the customer funnel will eventually lead to more users being passed through the rest of the customer funnel\n",
    "    - In unequal control/treatment groups, the number of users in the group will affect the total number of clicks\n",
    "- **CTR: $\\frac{\\text{Number of clicks}}{\\text{Number of page views}}$**\n",
    "    - Called Click-Through-Rate\n",
    "    - Single users can click more than once and inflate the CTR\n",
    "- **CTP: $\\frac{\\text{Unique visitors who click}}{\\text{Unique vistors to the page}}$**\n",
    "    - Called Click-Through-Probability\n",
    "    - The better metric to use in this case.\n",
    "    \n",
    "**Updated metric**\n",
    "\n",
    "Changing the \"Start Now\" button from orange to pink will **increase** the Click-Through-Probability of the button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When do you use CTR vs. CTP?**\n",
    "\n",
    "Generally:\n",
    "\n",
    "- Use a rate when you want to measure **usability**\n",
    "    - Users have a number of different places they can press, you use a rate to measure how often the users clicked a specific button\n",
    "    - Will have to change the website to log every page view of the website and every click of a button\n",
    "- Use a probability when you want to measure **total impact**\n",
    "    - You don't want to count when users double clicked, reloaded etc when measuring a total effect (e.g. getting to the second level of a page)\n",
    "    - Will have to change the website to match each page view with all of their \"child clicks\" to count at most one click per page view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which distribution?**\n",
    "\n",
    "When producing the CTP, the sample proportion $p_0$ was computed to be 0.1 $(\\frac{100}{1000})$. When using a different sample to compute the CTP, the sample proportion was instead computed to be 0.15.\n",
    "\n",
    "Is 0.15 or 15% considered to be surprising? How do you know how variable your estimate is likely to be?\n",
    "\n",
    "We compare the sample proportion computed to the **binomial** distribution where we model each click as a bernoulli trial. Each unique visitor either clicks the button (success) or doesn't (failure).\n",
    "\n",
    "**Variance**\n",
    "\n",
    "We can use the standard error formula for a sample proportion $SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$ to estimate how variable the sample proportion $p_0$ should be as a result of sampling variability. Then we compare the the second computed sample proportion (0.15) to the first relative to the variance to see if it is a surprising value or not. \n",
    "\n",
    "Either compute a CI or perform a hypothesis test\n",
    "\n",
    "**Practical significance**\n",
    "\n",
    "We have to decide how big of a change is practically significant (aka substantive) to warrant changing the existing system. Statistical significance of any arbitrary difference in proportion can be achieved with a big enough sample size, however a small difference may not be practically significant. \n",
    "\n",
    "- Each change may require an investment in resources and so a small change may not warrant the investment\n",
    "- Online A/B tests have a smaller margin for practical significance\n",
    "- We need to make sure for online A/B tests that the change is **repeatable**. - We want a big enough sample size to have it so that the statistical significance bar is lower than the practical significance bar to ensure repeatability\n",
    "\n",
    "We will decide that a 2% change in CTP is practically significant\n",
    "\n",
    "**Size vs. Power tradeoff**\n",
    "\n",
    "The power of a hypothesis test is the probability that the test rejects the null hypothesis  $H_0$  when a specific alternative hypothesis  $H_1$  is true. The idea is that, given a practically significant effect size and a significance level, we want the hypothesis test to be able detect the effect (by rejecting the null hypothesis) at a high enough probability, which can be controlled by increasing the sample size.\n",
    "\n",
    "$\\beta$ is the probability of making a type 2 error (failing to reject the null hypothesis when it is false). Statistical power is equal to $1-\\beta$\n",
    "\n",
    "When the CI captures the null hypothesis $H_0$, the test is statistically insignificant (recall that you create a CI around the point estimate $\\hat{p}$ or $\\hat{d} = \\hat{p}_1 - \\hat{p}_2$). When the CI is outside of both $H_0$ **and** the practical significance level $d_{\\text{min}}$, then we can agree to launch the change. For cases inbetween where the CI is too wide or does not capture $H_0$ but does capture $d_{\\text{min}}$, we have to use our best judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
