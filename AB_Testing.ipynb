{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AB Testing Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "A/B testing is a general methodology used to test out a new product or feature. A general overview of the procedure is as follows:\n",
    "\n",
    "- Take two sets of users\n",
    "- One set is shown an existing product\n",
    "- Second set is given a treated version\n",
    "- How do the customers respond differently? determine which one is better based on some metric\n",
    "\n",
    "Can you use A/B tests for everything? No.\n",
    "\n",
    "A/B testing is good for optimizing an existing product but not good for developing a new product based on an existing one.\n",
    "\n",
    "A few examples of big companies doing A/B tests:\n",
    "\n",
    "- Amazon did A/B tests for personalized recommendations and found that they had an increase in revenue when given the personalized recommendations.\n",
    "- Google tested 41 different shades of blue to use on their website using A/B tests\n",
    "- LinkedIn A/B tested a ranking process where they checked whether it's better to show news articles or an encouragement to add more contacts on a users \"stream\".\n",
    "- Amazon determined that every 100ms increase in page load time decreased sales by 1% by using A/B tests\n",
    "\n",
    "For all A/B tests you need a consistent measurable response from your control and experiment groups to determine whether the experience was improved (or made worse) by the treatment.\n",
    "\n",
    "### What can't you do with AB tests?\n",
    "\n",
    "- **Test out brand new experiences**\n",
    "    - Change aversion - Users refuse to participate in the test\n",
    "    - Novelty effect - Too drastic of a change leads customers to \"try out everything\". Can't test out a specific treatment\n",
    "- **No baseline for comparison**\n",
    "    - Can't set up a control group if there is no baseline.\n",
    "    - Same as the first point\n",
    "- **A short term decision**\n",
    "    - You need the plateaued experience to make a \"robust decision\". The metric being observed will be noisy in the beginning and only when it has stabilised can you check for any statistically significant changes to your metric. Thus you need time to complete A/B tests\n",
    "- **Long term effects are difficult to test**\n",
    "    - It is difficult to measure changes in your metric over a long time period where other aspects of your product or users will change (can't attribute the change in your metric to the treatment)\n",
    "- **Can't test whether your missing something in your product**\n",
    "    - There is no treatment for a \"missing feature\"\n",
    "    - Can't set up a control and treatment group because, what do you change about the treatment group?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques other than A/B tests\n",
    "\n",
    "- Logs of what users did on your website. Analyse them retrospectively or observationally to see if a hypothesis can be developed about what caused changes in their behaviour. This can then be used to design an experiment. \n",
    "- User experience research, focus groups, surveys, human evaluation\n",
    "- A/B testing gives quantitative data, other techniques give qualitative data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In online A/B tests, you don't know much about your users. You're using online user data and so it's difficult to distinguish whether a user is a single person, internet cafe etc.\n",
    "\n",
    "The goal is to determine whether a new feature is desirable. To do this, you need to design an experiment that can be **repeatable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Case Study\n",
    "\n",
    "Audacity\n",
    "\n",
    "Creates online finance courses\n",
    "\n",
    "**User flow/Customer funnel**\n",
    "\n",
    "<img src=\"images/Customer_Funnel.png\" width=600>\n",
    "\n",
    "Customers who start at the top of the funnel wil be passed down with more and more users dropping out along the way.\n",
    "\n",
    "**The hypothesis:**\n",
    "\n",
    "Changing the \"Start Now\" button from orange to pink will **increase** how many students explore Audacity's courses\n",
    "\n",
    "**Possible metrics to use**\n",
    "\n",
    "- **Total number of courses completed**\n",
    "    - Will take too much time. Students make take months to complete a course\n",
    "- **How many users click on the \"Start Now\" button**\n",
    "    - Assumes that users who progress through the top of the customer funnel will eventually lead to more users being passed through the rest of the customer funnel i.e. not students will drop out of the funnel along the way\n",
    "    - An unequal sizing of treatment and control groups will also lead to different total number of clicks. Doesn't just measure student's exploration of courses.\n",
    "- **CTR: $\\frac{\\text{Number of clicks}}{\\text{Number of page views}}$**\n",
    "    - Called Click-Through-Rate\n",
    "    - Single users can click more than once and inflate the CTR\n",
    "- **CTP: $\\frac{\\text{Unique visitors who click}}{\\text{Unique vistors to the page}}$**\n",
    "    - Called Click-Through-Probability\n",
    "    - The best metric to use in this case.\n",
    "    \n",
    "Changing the \"Start Now\" button from orange to pink will **increase** the Click-Through-Probability of the button. We assume an increase probability of clicking the button leads to more students exploring Audacity's courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use CTR vs. CTP?**\n",
    "\n",
    "Generally:\n",
    "\n",
    "- Use a rate when you want to measure **usability**\n",
    "    - Users have a number of different places they can press, you use a rate to measure how often the users clicked a specific button\n",
    "    - Will have to change the website to log every page view of the website and every click of a button\n",
    "- Use a probability when you want to measure **total impact**\n",
    "    - You don't want to count when users double clicked, reloaded, etc when measuring a total effect (e.g. getting to the second level of a page)\n",
    "    - Will have to change the website to match each page view with all of their \"child clicks\" to count at most one click per page view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which distribution?**\n",
    "\n",
    "Say for example the CTP was computed to be $p_1 = \\frac{100}{1000} = 0.1$ for one particular sample. When using a different sample, the CTP was instead computed to be $p_2 = 0.15$. \n",
    "\n",
    "Is $0.15$ considered to be a surprising result? Could this difference in proportion be a result of sampling variation? How inherently variable are our sample computed CTPs anyway? \n",
    "\n",
    "A standard result of statistical inference states that if the [Success-Failure Conditions](https://www.statisticshowto.datasciencecentral.com/success-failure-condition/) are satisfied then, assuming the two proportions $p_1$ and $p_2$ are the same, the **difference of two proportions** (in our case $\\hat{d} = p_2 - p_1 = 0.05$) should be distributed normally around $0$ with a given **Standard Error**. We can then perform a **Hypothesis Test** or compute a **Confidence Interval** to verify statistically whether there is actually a difference between the two proportons or not. If there is a difference then we reject the notion that the two proportions $p_1$ and $p_2$ are the same.\n",
    "\n",
    "Mathematically, this is written as \n",
    "\n",
    "$$H_0: \\hat{d} = p_1 - p_2 = 0$$\n",
    "$$H_A: \\hat{d} = p_1 - p_2 \\neq 0$$\n",
    "$$\\hat{d} \\sim N(0,SE)$$\n",
    "where\n",
    "$$SE_{d} = \\sqrt{\\frac{p_{pooled}(1-p_{pooled})}{n_1} + \\frac{p_{pooled}{(1-p_{pooled}})}{n_2}}$$\n",
    "$$p_{pooled} = \\frac{Number\\,of\\,successes}{Number\\,of\\,cases}$$\n",
    "\n",
    "The full mathematical procedure for computing a confidence interval is outlined below\n",
    "\n",
    "<img src=\"images/difference_of_two_proportions.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical significance**\n",
    "\n",
    "We have to decide how big of a change is practically significant (aka substantive) to warrant changing the existing system. Statistical significance of any arbitrary difference in proportion can be achieved with a big enough sample size, however a small difference may not be practically significant. \n",
    "\n",
    "- Each change may require an investment in resources and so a small change may not warrant the investment\n",
    "- Online A/B tests have a smaller margin for practical significance\n",
    "- We need to make sure for online A/B tests that the change is **repeatable**.\n",
    "- We want a big enough sample size to have it so that the statistical significance bar is lower than the practical significance bar to ensure repeatability\n",
    "\n",
    "We will decide that a 2% change in CTP is practically significant\n",
    "\n",
    "**Size vs. Power tradeoff**\n",
    "\n",
    "The power of a hypothesis test is the probability that the test rejects the null hypothesis  $H_0$  when a specific alternative hypothesis  $H_1$  is true. The idea is that, given a practically significant effect size and a significance level, we want the hypothesis test to be able detect the effect (by rejecting the null hypothesis) at a high enough probability, which can be controlled by increasing the sample size.\n",
    "\n",
    "$\\alpha$ is the probability of making a type 1 error i.e. False Positive (rejecting a true null hypothesis). $\\beta$ is the probability of making a type 2 error (failing to reject a false null hypothesis). **Statistical Power** is equal to $1-\\beta$\n",
    "\n",
    "When the CI captures the null hypothesis $H_0$, the test is statistically insignificant (recall that you create a CI around the point estimate $\\hat{p}$ or $\\hat{d} = \\hat{p}_1 - \\hat{p}_2$). When the CI is outside of both $H_0$ **and** the practical significance level $d_{\\text{min}}$, then we can agree to launch the change. For cases inbetween where the CI is too wide or does not capture $H_0$ but does capture $d_{\\text{min}}$, we have to use our best judgement. More on these decisions and their subsequent recommendations later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ethics\n",
    "\n",
    "Participants in any kind of experimental test need to be adequately protected\n",
    "\n",
    "**Example**\n",
    "\n",
    "Facebook experiment to gauge the effect of altering user's news feeds on emotions. In particular for this study, there is no discussion by the experimenters on the benefits of the study being conducted.\n",
    "\n",
    "Four main principles govern the ethics of experiments with regard to the safety of their participants:\n",
    "\n",
    "1. Risk - What risks are the participants being exposed to?\n",
    "2. Benefit - What benefits might be the outcome of the study?\n",
    "3. Choice - What other choices do participants have?\n",
    "4. Privacy - What expectation and confidentiality do participants have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risk**\n",
    "\n",
    "Does the risk exceed that of \"minimal risk\". **Minimal risk** is defined as the probability (and magnitude) of harm that a participant would be exposed to in normal daily life. In most online A/B testing, the risk (of the test) does not exceed minimal risk although there are grey areas such as in the Facebook example.\n",
    "\n",
    "**Benefits**\n",
    "\n",
    "How might the results of the experiment help? It is important to be able to state what the benefit would be from completing the study. In most online A/B testing, the befits are around improving the product.\n",
    "\n",
    "**Alternatives**\n",
    "\n",
    "Do participants of the test really have a choice in whether to participate or not (and how does that effect the risks and the benefits)?. For example, in medical clinical trials testing our new drugs for terminal illnesses, the alternative for most participants is death. Thus, the risk allowable for participants, given **informed consent**, is quite high.\n",
    "\n",
    "For online experiments, if users do not want to participate in the testing, we must consider how this may inconvenience the users (for example costs, time, information etc. required to switch services).\n",
    "\n",
    "**Data Sensitivity**\n",
    "\n",
    "How sensitive is the data? What is the re-identification risk of individuals from the data? As the sensitivity and the risk increases, then the level of data protection must increase: confidentiality, access control, security, monitoring & auditing, etc. Sensitive data includes bank information, health information etc. whilst the re-identification risk of the data is determined by whether it is considered to be identified, pseudonymous, anonymous or anonymized.\n",
    "\n",
    "**Identified data** means that data is stored and collected with personally identifiable information. This can be names, IDs such as a social security number or driver’s license ID, phone numbers, etc. HIPAA is a common standard, and that standard has 18 identifiers (see the Safe Harbor method) that it considers personally identifiable. Device id, such as a smartphone’s device id, are considered personally identifiable in many instances.\n",
    "\n",
    "**Anonymous data** means that data is stored and collected without any personally identifiable information. This data can be considered **pseudonymous** if it is stored with a randomly generated id such as a cookie that gets assigned on some event, such as the first time that a user goes to an app or website and does not have such an id stored.\n",
    "\n",
    "In most cases, anonymous data still has time-stamps -- which is one of the HIPAA 18 identifiers. Why? Well, we need to distinguish between anonymous data and anonymized data. **Anonymized data** is identified or anonymous data that has been looked at and guaranteed in some way that the re-identification risk is low to non-existent, i.e. given the data, it would be hard to impossible for someone to be able to figure out which individual this data refers to. Often times, this guarantee is done statistically, and looks at how many individuals would fall into every possible bucket (i.e., combination of values).\n",
    "\n",
    "What this means is that anonymous data may still have high re-identification risk. **Aggregated data** is usually not sensitive\n",
    "\n",
    "For online A/B testing, questions that must be considered include:\n",
    "\n",
    "- Are users being informed about the data being gathered via a ToS or Privacy policy?\n",
    "- What user identifiers are tied to the data being gathered? are there any identified data being gathered?\n",
    "- What type of data is being collected? Any health or financial data?\n",
    "- What level of confidentiality and security is the data subject to? Is the access of data being logged and audited.\n",
    "\n",
    "**Informed consent**\n",
    "\n",
    "Participants are told about the risks that they may face if they take part in the study, what benefits might result, what other options they have, what data is being gathered and how that data is being handled. Typically informed consent is handled by giving participants a document detailing all of the aforementioned information and participants can then choose whether they want to participate or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "There are two main uses for metrics in A/B testing:\n",
    "\n",
    "- Invariant checking - The metrics that shouldn't change across your treatment and control. \n",
    "- Evaluation - To check whether the treatment group is \"performing better\" than your evaluation group\n",
    "\n",
    "Two types of Evaluation metrics\n",
    "\n",
    "- High level metrics\n",
    "- Well defined metric \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level metrics\n",
    "\n",
    "These usually relate to the business objective. They are not directly used to perform the A/B test but they help to decide on which metrics will eventually be used to do so.\n",
    "\n",
    "In a customer driven business, we can use a **customer funnel** to brainstorm the possible high level metrics that will be important.\n",
    "\n",
    "<img src=\"\">\n",
    "\n",
    "Each level in the customer funnel is a high level metric that can be used to answer questions such as \"what business objective are you tracking?\" \n",
    "\n",
    "You can categorize every metric into three different types:\n",
    "- Count\n",
    "- Rate\n",
    "- Probability\n",
    "\n",
    "You want the metric that you eventually use in an A/B test to be a KPI for your business, but these high level metrics still need to be transformed into formal definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficult metrics\n",
    "\n",
    "Some metrics will be difficult to use in an A/B test. This comes down to two specific reasons\n",
    "\n",
    "- **Time**: For some metrics, the data will just take too long to collect. It is also difficult to ensure the smooth operation of a long-term ongoing online experiment  \n",
    "- **Availability**: For other metrics, the data may not be readily available to use. The business may not have access to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other techniques for coming up with metrics\n",
    "\n",
    "There are other techniques you can use to help you get an understanding of your users, which you can use to come up with ideas for metrics, validate your existing metrics, or even brainstorm ideas of what you might want to test in\n",
    "your experiments in the first place. See [here](https://s3-us-west-2.amazonaws.com/gae-supplemental-media/additional-techniquespdf/additional_techniques.pdf)\n",
    "\n",
    "<img src=\"images/gathering_additional_data.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well defined metrics\n",
    "\n",
    "To convert a high level metric into a well defined metric, you must consider two mains things:\n",
    "\n",
    "- What data are we actually going to look at to compute the metric (cookies? page views? clicks? are we going to filter the data?)\n",
    "- Given the events (e.g. clicks) how will we summarize the metric? (mean? median? etc)\n",
    "\n",
    "Essentially you must now consider the logistics of collecting and summarizing the metric\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\text{CTP} = \\frac{\\text{Unique visitors who click}}{\\text{Unique vistors to the page}}$\n",
    "\n",
    "This is not a well defined metric yet\n",
    "\n",
    "- how do we determine when two events are from the same user? \n",
    "    - Let's use a cookie\n",
    "- What time period do we use to count events? hour? day? week?\n",
    "\n",
    "\n",
    "A few well defined metrics that act like CTP are:\n",
    "\n",
    "1 - $\\text{Cookie Probability} = \\frac{\\text{Number of cookies that click}}{\\text{Total number of cookies}}$ for every hour (or any other time interval)\n",
    "\n",
    "2 - $\\text{Pageview probability} = \\frac{\\text{Pageview with click}}{\\text{Total number of page views}}$ for every hour (or any other time interval)\n",
    "\n",
    "3 - $\\text{CTR (with time period)} = \\frac{\\text{Total number of clicks}}{\\text{Total number of page views}}$ for every hour (or any other time interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting and filtering data\n",
    "\n",
    "Often times you want to filter out any **unexpected** traffic or participants from your experiments. An example would be spam IP addresses that don't represent a typical user (and thus you do not care how a change to your website affects these participants and thus do not want these participants to skew your metric). \n",
    "\n",
    "Filtering out changes that are targeted towards a specific subset of users is also important because you want to avoid **diluting your results**. Results are diluted when a change in your metric can no longer be detected due a **reduction in power** of the A/B test where you have included participants in your experiment whom you know will not be affected. $N$ is then much larger than it should be, and legitimate changes to the behaviour of the participants (that you suspect should be affected) may be considered as **sampling variation** in the hypothesis test.\n",
    "\n",
    "Filtering is used to **de-bias** the data whilst avoiding **introducing** bias to the data. Bias may be introduced if you choose to use a filter but it is removing participants from certain subsets disproportionately. The experiment is no longer **randomized**.\n",
    "\n",
    "Use **slicing** to check if your filter is introducing bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary metrics\n",
    "\n",
    "For count metrics in particular and other well defined metrics (e.g. load time of a video) you have many different ways to summarize the metric e.g. mean, median, 50%ile, 75%ile, 90%ile etc. \n",
    "\n",
    "To choose between these metrics, you should consider two main factors:\n",
    "\n",
    "- **Sensitivity** - You want your metric to be sensitive enough to changes in the treatment\n",
    "- **Robustness** - You want your metric to be robust to ongoing changes in your system that aren't the treatment but you have no control over\n",
    "\n",
    "To choose how to summarize the metric, you can perform a **retrospective analysis** and produce a **histogram** of the metric. For skewed distributions, the mean is not necessarily the best measure of central tendency and a median or %ile may be more appropriate.\n",
    "\n",
    ">Let’s talk about some common distributions that come up when you look at real user data. For example, let’s measure the rate at which users click on a result on our search page, analogously, we could measure the average staytime on the results page before traveling to a result. In this case, you’d probably see what we call a **Poisson distribution**, or that the stay times would be exponentially distributed. Another common distribution of user data is a “power-law,” Zipfian or **Pareto distribution**. That basically means that the probability of a more extreme value, z, decreases like 1/z (or 1/z^exponent). This distribution also comes up in other rare events such as the frequency of words in a text (the most common word is really really common compared to the next word on the list). These types of heavy-tailed distributions are common in internet data. Finally, you may have data that is a composition of different distributions - latency often has this characteristic because users on fast internet connection form one group and users on dial-up or cell phone networks form another. Even on mobile phones you may have differences between carriers, or newer cell phones vs. older text-based displays. This forms what is called a **mixture distribution** that can be hard to detect or characterize well. The key here is not to necessarily come up with a distribution to match if the answer isn’t clear - that can be helpful - but to choose summary statistics that make the most sense for what you do have. If you have a distribution that is lopsided with a very long tail, choosing the mean probably doesn’t work for you very well - and in the case of something like the Pareto, the mean may be infinite!\n",
    "\n",
    "There are 4 main types of summary metrics\n",
    "\n",
    "<img src=\"images/categories_for_summary_metrics.png\" width=600>\n",
    "\n",
    "A review of the literature may need to be considered to select the best summary metric (e.g. say a study showed that people take 5 seconds to internalize the information of a web page, then use the number of users who spent 5 seconds or longer on a page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity or Robustness\n",
    "\n",
    "The mean is **not a robust metric** as it is sensitive to outliers, the mean of a metric is heavily influnced by values that proportional to their size. The median is robust but may not be sensitive enough.\n",
    "\n",
    "To measure the **sensitivity** of a summary metric, you can perform a **retrospective study** and look at previous changes to the website (that match the change you're trying to test) to see if it affected the metric in ways that you expect. You can also perform separate simple experiments to check if your summary metric responds to the change in expected ways. Looking back at previous experiments performed may also be done.\n",
    "\n",
    "To measure the **robustness** of a summary metric, you can perform an **A/A test** where you don't change anything and see if the metric picks up on any spurious changes. The metric is not robust if there are more statistically significant changes to your metric between A/A groups than expected due to sampling variation. (See the spreadsheet for example). You can also perform retrospective studies or simple experiments and check to see if your metric is robust between comparable groups.\n",
    "\n",
    "Here is an example of a simple experiment to check for robustness where each video should be comparable in latency (each video is the same size, resolution etc). \n",
    "\n",
    "<img src=\"images/robustness.png\" width=600>\n",
    "\n",
    "\n",
    "Here is an example of a simple experiment to check for sensitivity where the higher the video number, the lower the resolution (i.e. we expect the latency to decrease w.r.t video number)\n",
    "\n",
    "<img src=\"images/sensitivity.png\" width=600>\n",
    "\n",
    "**From the experiments, the 85th percentile might be a good metric to use based on sensitivity and robustness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute vs Relative difference\n",
    "\n",
    "How do you compute the difference in your metric between treatment and control? If you run multiple experiments, then using a relative difference (e.g. percentage change) means that you only need to define one practical effect size (e.g. 2% increase).\n",
    "\n",
    "Absolute vs. relative difference\n",
    "\n",
    "Suppose you run an experiment where you measure the number of visits to your homepage, and you measure 5000 visits in the control and 7000 in the experiment. Then the absolute difference is the result of subtracting one from the other, that is, 2000. The relative difference is the absolute difference divided by the control metric, that is, 40%.\n",
    "\n",
    "Relative differences in probabilities\n",
    "\n",
    "For probability metrics, people often use percentage points to refer to absolute differences and percentages to refer to relative differences. For example, if your control click-through-probability were 5%, and your experiment click-through-probability were 7%, the absolute difference would be 2 percentage points, and the relative difference would be 40 percent. However, sometimes people will refer to the absolute difference as a 2 percent change, so if someone gives you a percentage, it's important to clarify whether they mean a relative or absolute difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variability\n",
    "\n",
    "You need to compute the variance of the metric in order to check whether a statistically (or practically) significant difference in your metric has resulted from the treatment.\n",
    "\n",
    "For some non-standard metrics (e.g. ratios, 90%ile) it is difficult to calculate the variance analytically, and so it is better to compute the variance empirically. Here is a list of common metrics and their sample distribution/sample variance\n",
    "\n",
    "<img src=\"images/metrics_and_variance.png\" width=600>\n",
    "\n",
    "For some metrics (e.g. median, Xth percentiles, ratios, etc), it is difficult to calculate the variance analytically because of complicated SE formulas and assumptions that are required. You may also not want to make any assumptions about the distribution of these kinds of metrics. In these cases, there are two simple solutions to help you proceed with performing the A/B test:\n",
    "\n",
    "- **Sign test** - For example, run multiple experiments (e.g. 20 experiments) using the same sample size $N$. If on 15 of those experiments the metric increased, use the binomial distribution to perform a hypothesis test to verify the chance of this happening due to sampling variation (assume $p=0.5$ is the probability that there is an increase in the metric).\n",
    "    - Doesn't help estimate the size of the effect (practical significance)\n",
    "    - Part of a broader range of methods called non-parametric methods\n",
    "\n",
    "- **Emperically compute SE and CI** - This can be done by again running multiple experiments using the same sample size $N$, but instead we do not administer a treatment (i.e. there is no difference between the control and treatment groups). We then compute the standard deviation of the metric and use that as an **unbiased estimate** of the Standard Error of our metric. This type of test is called an A/A test.\n",
    "    - Can be done beforehand to estimate the SE of our metric\n",
    "    - We can extrapolate SE to different sample sizes N, thus A/A tests can be done way before an A/B tests is ready\n",
    "\n",
    "See the [spreadsheet](Emperical_Variance_Notes.xlsx) for an example of performing A/A tests to empirically compute Confidence Intervals and estimate the Standard Errors for CTP. \n",
    "\n",
    "If you don't have the time to perform multiple small experiments, you can instead perform one large experiment and estimate the CI and SE by sampling with replacement from the large experiment to simulate performing multiple smaller experiments. This technique is called **bootstrapping**\n",
    "\n",
    "See this [spreadsheet](Empiral_Variance_Bootstrapping.xlsx) for an example of performing bootstrapping to empircally estimate CI and SE of CTP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing an Experiment\n",
    "\n",
    "### Units of diversion\n",
    "\n",
    "In A/B testing, we need to assign different subjects to our control and treatment groups. How do you decide what is a subject in the experiment? With a user visible change, ideally we want a unique person to be a subject in our experiment, however there only exists imperfect proxys for a unique person in online testing. The way we define/approximate a subject in our experiment is called the **unit of diversion** where you divert each unit into the control or treatment groups.\n",
    "\n",
    "We may try to distinguish users by\n",
    "\n",
    "- **User ID**\n",
    "    - A single user may sometimes have multiple accounts\n",
    "    - is personally identifiable\n",
    "- **Cookie** \n",
    "    - If you switch browser, you get assigned a different cookie\n",
    "    - If you clear your history, you get assigned a different cookie\n",
    "- **Event**\n",
    "    - Users will not get a consistent experience\n",
    "- **Device ID**\n",
    "    - Only available for mobile\n",
    "    - tied to a specific device\n",
    "    - is personally identifiable\n",
    "- **IP address**\n",
    "    - Can change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency of diversion\n",
    "\n",
    "There are three main things to consider when selecting the unit of diversion\n",
    "\n",
    "**Consistency and Statefulness**\n",
    "\n",
    "The idea is to make sure that the subjects in your experiment has a consistent experience during testing i.e. the experience of each subject is the same throughout the lifetime of the experiment.\n",
    "\n",
    "For user visible changes, it's best to use User ID or Cookies to ensure consistency of diversion\n",
    "\n",
    "For non user visible changes e.g. latency changes, backend infrastructure changes, ranking changes etc, you do not need to worry about consistency and thus you can used an event based unit of diversion\n",
    "\n",
    "\n",
    "If the metric you're measuring is some kind of learning effect i.e. if a user adapts to changes made, then you need to track the same user over a long period of time. You then need to use a **stateful** unit of diversion that is attached to a single user for which User ID and Cookie is most appropriate.\n",
    "\n",
    "**Ethical considerations**\n",
    "\n",
    "Cookie and User ID are personally identifiable units of diversion. To use these then, you need to consider whether your prepared to go through all of the necessary data protection steps that are required when collecting such data. This includes for example collecting informed consent from your participants.\n",
    "\n",
    "**Variability**\n",
    "\n",
    "When the **unit of analysis** is the same as the unit of diversion, you have a lower **empirical variability** in your metric due to sampling variation (which is desirable as it increases the power of the A/B test). This means that the analytical estimate of the variance of your metric is an overestimate. The unit of analysis is the denominator of the metric (if your metric is a ratio).\n",
    "\n",
    "You then want to choose a **unit of diversion** that matches the denominator of your metric when you can (which usually ends up being an event based diversion).\n",
    "\n",
    "<img src=\"images/unit_of_diversion_unit_of_analysis.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter vs. Intra user experiment\n",
    "\n",
    "A/B testing is an inter user experiment where there are different participants in each of the control and treatment groups. More information in [this paper](http://www.cs.cornell.edu/people/tj/publications/chapelle_etal_12a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target population\n",
    "\n",
    "There are various reasons why you would want to target a specfic group in the population. This is particularly important if you know in advance who will be affected by the changes made in your treatment group. Some reasons to target a specific group in your population include (but is not limited to):\n",
    "\n",
    "- If you're testing a high-profile feature that you're unsure about whether it will be released or not, you may want to restrict the change so that only a limited amount of users experience it and as a result avoid getting press coverage etc.\n",
    "- You may want to restrict the target population by language as you want to avoid going through the trouble of testing in different languages.\n",
    "- You may not be sure whether your feature works on all browsers and thus you may want to restrict the target population to specific browsers\n",
    "- If you're running multiple experiments at your company, you may not want to overlap participants of the study (have subjects take part in multiple experiments)\n",
    "- You may not want to dilute the effect of your experiment if you know your change will only affect a specific group of your population. Increasing $N$ whilst keeping the number of participants affected the same will reduce the effect size i.e. the difference between proportions for CTP and thus reduce the power of your experiment (see filtering section)\n",
    "\n",
    "You should to ask the engineering team if they have an idea about who the changes will target.\n",
    "\n",
    "After the experiment, you will want to test the changes on your global population just to check if there are any unwanted effects on the traffic you were not targetting. An example of diluting the effect size is shown below:\n",
    "\n",
    "<img src=\"images/diluting_variance_1.png\" width=600>\n",
    "\n",
    "<img src=\"images/diluting_variance_2.png\" width=600>\n",
    "\n",
    "Adding all of the unaffected traffic outside of the population that would be affected (New Zealand traffic) diluted the difference of the poportions (disproportionately to the reduction in SE) and thus the difference is no longer statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohort\n",
    "\n",
    "If you just divert your users by Cookie or User ID, you may have participants drop out of your groups or join your groups during the lifetime of your experiment. Cohorts are groups of users who entered the experiment at the same time and/or have a similar level of activity. You typically want to use a cohort in experiments when:\n",
    "\n",
    "- You're looking for learning effects (whether users are adapting to a change or not)\n",
    "- Examining user retention\n",
    "- Want to increase user activity\n",
    "- Anything requiring the user to be established i.e. has used the site for at least a specific number of hours\n",
    "\n",
    "This is essentially a form of filtering to ensure that the participants of the study match the target population of the test. In the audacity case study, they may change the structure of a specific course and only target a cohort of participants who haven't completed the course yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing the experiment\n",
    "\n",
    "Before we only considered how to size the experiment based on practical significance, statistical significance and sensitivity (e.g. ensuring the experiment has at least 60% power). From what we just learned, we now also need to take into account the unit of diversion vs. unit of analysis as well as the target population and then decide whether the size is realistic relative to how long you have to run the experiment.\n",
    "\n",
    "To perform the calculation, you just do a power calculation for $N$, the sample size required to ensure a certain $\\alpha$ (usually 0.05), $\\beta$ (usually 0.2) and $d_{\\text{min}}$ whilst estimating SE empirically (or analytically for simple metrics).\n",
    "\n",
    "To reduce the size of the experiment required whilst maintaining the same $\\alpha$, $\\beta$ and $d_{\\text{min}}$, you could try:\n",
    "- Changing the unit of diversion to match the unit of analysis\n",
    "- Target experiment to specific traffic (filtering or using cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing triggering\n",
    "\n",
    "Even after deciding on a well-defined metric, you often don't know beforehand whether the population that is supposed to be exposed to the changes in your treatment are truly exposed to the changes or not. A good idea is to run a \"pilot\" where you turn on the changes and observe who actually are affected and see if it matches your intutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration\n",
    "\n",
    "Depending on the sizing calculations relative to the traffic on your website, you will need to decide on a suitable **duration** and **exposure proportion**.\n",
    "\n",
    "For example, if your website gets $100000$ users per day, and the sizing calculations resulted in a required sample size of $N=1000000$, if you expose your change to your website's entire traffic, then you will need to run your experiment for 10 days.\n",
    "\n",
    "However, exposing the change to the entire traffic isn't a good idea because of reasons mentioned before in the \"Target Population\" section such as avoiding press coverage, overlapping experiments, filtering by cohorts etc.\n",
    "\n",
    "Reducing the exposure proportion will then increase the required duration of your experiment.\n",
    "\n",
    "Another reason is the fact that you are trying to account for confounding factors by **randomizing across your unit of diversion**. However, other **temporal factors** might be affecting your subjects/metric which you have to also account for. For example, if you run your online experiment over a holiday, the results of the experiment (if statistically and practically significant) may not apply all year round. \n",
    "\n",
    "By having a longer running experiment, or by running your experiment on specific days of the week/year, you can ensure that the results of your experiment are **generalizable**.\n",
    "\n",
    "<img src=\"images/limit_exposure.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning effects\n",
    "\n",
    "When you want to measure learning effects, you're trying to quantify whether users are adapting to the changes (in your treatment group) or not. Measuring this type of effect is difficult due to the two reasons mentioned before:\n",
    "\n",
    "**Change aversion** - Users refuse to participate in the test\n",
    "**Novelty effect** - Too drastic of a change leads customers to \"try out everything\". Can't test out a specific treatment\n",
    "\n",
    "The key idea is that for learning effects, you need to consider the duration of the experiment that is required for the user behaviour to plateau or converge.\n",
    "\n",
    "These are the things you need to keep in mind when measuring learning effects (some of which has been mentioned already)\n",
    "\n",
    "- You need a **stateful** unit of diversion\n",
    "- You need to consider the dosage (how often the user experiences the change) and thus you should select a **cohort** based on dosage\n",
    "- Learning effects are usually high risk and thus you'll probably want to test on a smaller proportion of users for a longer duration of time (to meet the sample size $N$ quota)\n",
    "\n",
    "**pre-period vs. post-period experiments** is a method you can use to ensure that user experience and populations are comparable between your control and treatment groups. The idea is that you run A/A tests **before and after** the actual A/B test on your treatment and control groups to ensure that there are no inherent differences between the two groups. This ensures that after your treatment group is exposed to the change, the differences between the two groups can be attributed to the treatment and not any other confounding factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks\n",
    "\n",
    "Before analyzing and interpreting the results of an A/B tests, you should do a few sanity checks to make sure your experiment was run properly. Specifically, you need to check that the **invariant metrics**, which are quantitative measures that you expect to be the same between the treatment and control groups, are actually the same. Invariant metrics come in two types:\n",
    "\n",
    "- **Population sizing** - Experiment and control groups should be comparable in size and types of users in each group\n",
    "- **Other invariants** - Other aspects of the experiment that isn't part of the treatment needs to be comparable between the two groups\n",
    "\n",
    "The total number of units of diversion  (e.g. total number of signed in Users) in each group is always a population sizing invariant as it defines the size of your treatment and control groups.\n",
    "\n",
    "Other invariants are those that measure an effect that subjects in both groups experience equally (e.g. CTR of a button that is the same between treatment and control)\n",
    "\n",
    "The metric you're testing (or quantities used to compute the metric) are not and should not be invariant as you expect them to change as a result of the treatment.\n",
    "\n",
    "<img src = \"images/checking_invariants.png\" width=600>\n",
    "\n",
    "To actually check if the invariant metrics are the same between the two groups, you perform the appropriate hypothesis test (e.g. **one sample proportion inference** for testing the total number of units of diversion in each group) or compute a confidence interval and see if it captures the null hypothesis.\n",
    "\n",
    "If your sanity checks fail, do not proceed with the conclusions of your A/B test. Here are a couple of solutions and/or next steps:\n",
    "\n",
    "- Check with the engineers to see if there is any problem with the infrastructure which is resulting in different population sizing\n",
    "- Perform a retrospective analysis that tries to recreate the experiment diversion (sample from a previous data capture that emulates the diversion used in the experiment) to see if there is something endemic about the experiment conditions i.e. perhaps the invariant metric is not invariant as expected\n",
    "- Perform pre-period and post-period A/A tests to check if there's something wrong with the treatment conditions (change in an invariant during the A/B test but not during pre-period) or if there's something that's systematically wrong with the setup or data capture (a change in an invariant in pre-period and post-period).\n",
    "\n",
    "Some common problems that causes invariants to change\n",
    "\n",
    "- **Data capture** - Capturing a new experience or something that happens rarely. Effects aren't being measured correctly or in the same way between your treatment and control groups\n",
    "- **Setup** - Having a filter (e.g. English traffic only) but the filter works differently between your treatment and control groups\n",
    "- **Infrastructure** - Something systematically wrong, for example resetting cookies for one group but not the other.\n",
    "\n",
    "Recall that invariants are what should be kept the same in both experiment groups, if the change is something that in reality shouldn't be an invariant and was previously not considered then it must now be accounted for in a new experimental setup (e.g by targetting a new specific population or cohort). \n",
    "\n",
    "Learning effects should appear to slowly deviate from invariance and so can be ruled out if there are big immediate changes to invariants between treatment and control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single metrics\n",
    "\n",
    "To evaluate the results of an A/B test on a single metric, you follow the standard statistical procedure of computing a confidence interval or performing a hypothesis test.\n",
    "\n",
    "Recall that for a **difference of two proportions** test, the following relationship holds:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "SE & \\sim \\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}} \\\\\n",
    "\\Rightarrow \\frac{SE}{\\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}}} & = \\frac{\\hat{SE}}{\\sqrt{\\frac{1}{\\hat{N_1}} + \\frac{1}{\\hat{N_2}}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $N_1$, $N_2$ are the total units of diversion in the control and experiment groups respectively, and the $\\hat{\\text{hat}}$ symbols are their respective quantities for a different sample size.\n",
    "\n",
    "This relationship states that, because of the proportionality between SE and the sizes of the samples used in the test, we can extrapolate the empirically computed SE of a previous A/A test to compute the SE of our metric in the actual A/B test.\n",
    "\n",
    "To see an example of analyzing the results of a single metric A/B test, see [this spreadsheet](Single_Metric_Example.xlsx). Use [this website](https://www.graphpad.com/quickcalcs/binomial1.cfm) to quickly perform a sign test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the results**\n",
    "\n",
    "You need to decide if you've observed a statistically significant change in your experiment metric. We also want to estimate the magnitude and direction of the change via a confidence interval. Once you have this information, you want to make a decision on whether you recommend that your business launches the change. \n",
    "\n",
    "Things to keep in mind when interpreting the results:\n",
    "\n",
    "- If there is no statistically significant change in the parametric test then try filtering or segmenting your target population e.g. by platform, days of the week, etc, to check for bugs or come up with a new hypothesis for who the affected population might be.\n",
    "- Cross check your results with the sign test to see if it agrees with the parametric test. If they don't agree, which of the experiments are not as expected? is there a trend in conditions for these particular experiments?\n",
    "    \n",
    "If you find your feature performing differently for a specific filter of your target population, then a phenomenon called [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox) may be occurring in which a trend appears in several different groups of data (e.g. increase in metric) but disappears or reverses when these groups are combined (e.g. decrease in metric).\n",
    "\n",
    "<img src=\"images/Simpsons_paradox.png\" width=600>\n",
    "\n",
    "It usually occurs when population sizing invariants are violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Metrics\n",
    "\n",
    "When performing multiple hypothesis tests, it becomes increasingly likely that you observe statistical significance purely by chance (due to sampling variation). As a result the significance level $\\alpha$ needs to be adjusted when analyzing the p-values of multiple hypothesis tests. See [this article](https://en.wikipedia.org/wiki/Multiple_comparisons_problem) for more details on multiple comparisons.\n",
    "\n",
    "<img src=\"images/multiple_metric_tests.png\" width=600>\n",
    "\n",
    "The **Family-Wise Error Rate (FWER)** is the **probability of making one-or-more false discoveries** (type 1 errors i.e. rejecting a true null hypothesis) and replaces $\\alpha$ when analyzing multiple p-values. Essentially the threshold shrinks such that $\\text{FWER} < \\alpha$ to account for multiple comparisons. You only declare a test as statistically significant when its p-value is below the FWER. One such method that controls the FWER is the **Bonferroni correction** which ensures that the FWER is at most equal to $\\alpha$. See [this article](https://en.wikipedia.org/wiki/Family-wise_error_rate) for more details on FEWR.\n",
    "\n",
    "The Bonferroni correction assumes that all of the p-values for the metrics being tested are **independent**. This results in too conservative of an estimate for the FEWR i.e. it's too low and reduces the power of your A/B tests too drastically. This likely occurs when you are monitoring more than one metrics that are likely to move in unison i.e. they're correlated.\n",
    "\n",
    "Another class of methods modifies $\\alpha$ to control the proportion of discoveries (rejected null hypotheses) that are false (type 1 errors). This proportion is the **expected proportion of false positives among all significant tests** and is called the **False Discovery Rate (FDR)**. Like FEWR you declare all hypothesis tests under the FDR to be statistically significant. One such method that controls the FDR is the **Benjamini–Hochberg procedure** which ensures that the FDR is at most equal to $\\alpha$. See [this article](https://en.wikipedia.org/wiki/False_discovery_rate) for more details on FDR.\n",
    "\n",
    "FDR-controlling procedures are a good solution when a huge number of A/B tests have been conducted. They have greater power, at the cost of increased numbers of Type I errors.\n",
    "\n",
    "Links to more advanced methods: [Closed Testing](https://en.wikipedia.org/wiki/Closed_testing_procedure) [Holm-Bonferroni](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method) [Boole's Inequality](https://en.wikipedia.org/wiki/Boole%27s_inequality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting multiple metrics\n",
    "\n",
    "What you're hoping for is that **related metrics** are going to move in the same direction, e.g. CTR and CTP should hopefully both increase. **Composite metrics** that are computed using similar quantities should also hopefully move in the same direction and analysing the formula for the composite metrics will hopefully tell you why they have moved in a particular direction. \n",
    "\n",
    "Multiple metrics however can be unruly. It's usually a better idea to eventually come up with a single value **Overall Evaluation Criteria OEC** as a KPI and use that as a measure of whether a desirable change has been made to the business.\n",
    "\n",
    "How do you come up with one?. It's usually a business decision where you have to decide how much you weight the improvement of particular metrics over the other. This is especially important when optimizing one metric compromises the other and thus you must come up with some OEC combination of multiple metrics that tells you, using a single number, that you're not improving one metric too much at the expense of another. See for example how [F1 score combines and weights two rate metrics (Precision and Recall)](https://en.wikipedia.org/wiki/F1_score#Definition) depending on which one the business finds more important.\n",
    "\n",
    "Having an OEC as a weighted sum of other metrics doesn't necessarily have to be used to make a launch decision. It can be instead used to decide which metrics the business finds important (and reweighting can be done appropriately).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions - Making a recommendation\n",
    "\n",
    "A summary of the possible scenarios, recommendations and conclusions that can be made after performing A/B tests:\n",
    "\n",
    "- **Interpreting the p-values of multiple metrics**\n",
    "    - Use sophisticated methods to control FEWR and FDR and correlation\n",
    "    - Discuss results with decision makers and launch non-risky changes or run further experiments\n",
    "- **If you have a statistically statistical change in some metrics, but for others you don't**\n",
    "    - Try to understand why this is happening. Should these changes actually be moving in unison? Is it OK for small changes in multiple metrics not to to move in unison? Is it OK for big changes not to move in unison? \n",
    "    - For example, improving the design of a page can counter-intuitively reduce the reading time of a page and increase the CTR/CTP of buttons. \n",
    "- **If you have a positive impact for one slice (e.g. English traffic) and no impact or a negative impact for another slice (e.g. Korean Traffic) (Simpson's paradox)**\n",
    "    - Again, try to undersand why this is happening. Initial intuitions of how different slices should react may be incorrect. Can you replicate the effect in an observational study? Do you have a bug?\n",
    "    - For example, boldfacing words in English provides emphasis but this is not the case for Chinese, Japanese, Korean since it makes words harder to read.\n",
    "    - Is there something wrong with the set-up of the experiment? are the different slices actually being exposed to the change in the same way?  \n",
    "- **If your parametric tests (e.g. difference of two proportions) and non-parametric tests (e.g. sign test) disagrees with eachother**\n",
    "    - which experiments in the sign tests didn't agree with the overall parametric test? can you modify the metric or target population?\n",
    "    - For example sign test tells you that there is no impact throughout the week but there is a postive impact on weekends. Parametric test tells you that there is a positive impact throughout the week \n",
    "- **Invariant sanity checks fail**\n",
    "    - Do not proceed with interpreting p-values or confidence intervals\n",
    "    - Perform pre-period and post-period A/A tests to check for problems in the setup, data capture or treatment (is the treatment being exposed properly to thw two groups?)\n",
    "    - Perform retrospective analysis to see if you can recreate the problem\n",
    "    - Consult engineers to check if there is problem with the infrastructure for population invariants\n",
    "- **When do you actually launch a change?**\n",
    "    - Have statistical and practically significant changes\n",
    "    - Do I understand why the change occurred?\n",
    "    - Is it worth it? i.e. how much does it cost to launch or maintain the change?\n",
    "    - Have you only ran 1 experiment? Should you run more than 1? How important is the change?\n",
    "- **Slicing reveals that 30% of users experience a positive impact whilst 70% experience no impact. Or 70% is improved whilst 30% is negatively impacted**\n",
    "    - Wait and fine tune the change?\n",
    "    - Launch it as is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting an A/B test over time\n",
    "\n",
    "If you decide to launch the changes of an A/B test, it is often a good idea to instead **slowly ramp up** the experiment i.e. increase the traffic diverted to the experiment group until it is eventually fully launched, for example start with 1% of traffic and increase this, if tests are passed, until 100% of traffic is diverted to the experiment group.\n",
    "\n",
    "You can slowly **remove filters** such that the target population is less and less segmented and becomes the entire traffic of the website. For example, test changes on English traffic, then English and French, then eventually all languages.\n",
    "\n",
    "However increasing the % of traffic diverted to the experiment group or removing filters will naturally dilute the effect size. Confounding factors start taking effect e.g. more student traffic, temporal factors such as holidays, etc.\n",
    "\n",
    "You can use a **hold-out** group which never see the change and continue comparing the effect size against that group to see if the dilution is also present in the hold-out to verify that the effect is still impactful and significant over time for the entire traffic.\n",
    "\n",
    "Novelty effect and Change Aversion can come into play and also start diluting effect size once you ramp up and remove filters. This is when cohort analysis is useful once again (targetting participants that see the change at the same time). Pre-Period and Post-Period are also useful to monitor how users behaviours change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Overview: Free Trial Screener\n",
    "\n",
    "Experiment Overview: Free Trial Screener\n",
    "At the time of this experiment, Udacity courses currently have two options on the course overview page: \"start free trial\", and \"access course materials\". If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n",
    "\n",
    "\n",
    "In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. [This screenshot](https://drive.google.com/file/d/0ByAfiG8HpNUMakVrS0s4cGN2TjQ/view) shows what the experiment looks like.\n",
    "\n",
    "\n",
    "The hypothesis was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time—without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n",
    "\n",
    "The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Design\n",
    "\n",
    "**Metric Choice**\n",
    "\n",
    "The following metrics will be used as **invariant metrics:**\n",
    "- **Number of cookies** - The number of unique cookies to view the course overview page\n",
    "    - This is the unit of diversion for participants who have not enrolled yet. Unit of diversion metrics are always invariant\n",
    "- **Number of clicks** - The number of unique cookies to click the \"Start free trial\" button (which happens before the free trial screener is triggered)\n",
    "    - This metric measures an effect that is experienced by users in both groups equally. The \"Start free trial\" button will not change in the experiment.\n",
    "- **CTP** - $\\frac{\\text{Number of unique cookies to click the \"Start free trial\" button}}{\\text{Number of unique cookies to view the course overview page}}$\n",
    "    - Another metric that measures an effect that is the same for both groups. Since the \"Start free trial\" button is the same for both groups, the CTP should also remain the same between both groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide on the evaluation metrics to use, we must keep in mind what the hypothesis of the experiment is.\n",
    "\n",
    "The following metrics will be used as **evaluation metrics:**\n",
    "- **Gross Conversion** - $\\frac{\\text{Number of User-IDs to complete checkout and enroll in the free trial}}{\\text{Number of unique cookies to click the \"Start free trial\" button}}$\n",
    "    - This measures an effect that is expected to be different between the experiment and control groups so it will not be invariant. \n",
    "    - The experiment group will see the screener page and the control group will not, thus the number of User-IDs who enroll should be different between the two groups.\n",
    "    - According to the hypothesis this metric should decrease. By setting clearer expectations for students upfront, only students who are committed to completing the course should enroll in the free trial (numerator decreases, denominator stays the same).\n",
    "- **Retention** - $\\frac{\\text{Number of User-IDs to remain enrolled past the 14-day boundary (and thus make at least one payment)}}{\\text{Number of User-IDs to complete checkout and enroll}}$\n",
    "    - This measures an effect that is expected to be different between the experiment and control groups so it will not be invariant.\n",
    "    - The experiment group will see the screener page and the control group will not, thus the number of User-IDs who remain enrolled as well as the Number of User-IDs to complete checkout should be different between the two groups.\n",
    "    - According to the hypothesis this metric should increase. If only students who are committed to completing the course complete checkout then the idea is, a larger proportion of these students should remain enrolled (numerator increases, denominator decreases).\n",
    "- **Net conversion** - $\\frac{\\text{Number of User-IDs to remain enrolled past the 14-day boundary (and thus make at least one payment)}}{\\text{Number of unique cookies to click the \"Start free trial\" button}}$\n",
    "    - This measures an effect that is expected to be different between the experiment and control groups so it will not be invariant.\n",
    "    - The experiment group will see the screener page and the control group will not, thus the number of User-IDs who remain enrolled as well as the number of unique cookies to click the \"Start free trial\" button should be different between the two groups.\n",
    "    - According to the hypothesis this metric should increase. If students have their expectations set early then the number of User-IDs to remain enrolled should increase (numerator increase, denominator stay the same)\n",
    "    - In reality, it is **unclear** whether this metric will increase as the fraction is out of all cookies who click the \"Start free trial\" button. The number of students enrolling could decrease due to only committed students enrolling, which would decrease the overall number of students who remain enrolled (the numerator) but still increase the **Retention** rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating standard deviation**\n",
    "\n",
    "Previous analysis shows the following baseline values for the following metrics:\n",
    "\n",
    "- Unique cookies to view course overview page per day - 40000\n",
    "- Unique cookies to click \"Start free trial\" per day - 3200\n",
    "- Enrollments per day - 660\n",
    "- CTP on \"Start free trial\" button - 0.08\n",
    "- Probability of enrolling, given click - 0.20625\n",
    "- Probability of remaining enrolled, given enroll - 0.53\n",
    "- Probability of payment, given click - 0.1093125\n",
    "\n",
    "Standard Error of a proportion:\n",
    "$$SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$$\n",
    "\n",
    "Figure out how many units of analysis will correspond to 5000 cookies for each metric. \n",
    "\n",
    "$\\text{Number of unique cookies to click the \"Start free trial\" button} = 5000 \\times 0.08 = 400$\n",
    "\n",
    "$\\text{Number of User-IDs to complete checkout and enroll} = 400 \\times 0.20625 = 82.5$\n",
    "\n",
    "Substitute for $N$ and $p$ (the metric's corresponding probability) in the SE formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0202\n"
     ]
    }
   ],
   "source": [
    "# Gross conversion SE\n",
    "total_num_UOA = 5000 * 0.08\n",
    "print(\"{:.4f}\".format(np.sqrt((0.20625*(1-0.20625))/total_num_UOA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0549\n"
     ]
    }
   ],
   "source": [
    "# Retention SE\n",
    "total_num_UOA = 5000 * 0.08 * 0.20625\n",
    "print(\"{:.4f}\".format(np.sqrt((0.53*(1-0.53))/total_num_UOA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0156\n"
     ]
    }
   ],
   "source": [
    "# Net conversion SE\n",
    "total_num_UOA = 5000 * 0.08\n",
    "print(\"{:.4f}\".format(np.sqrt((0.1093125*(1-0.1093125))/total_num_UOA)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gross Conversion, the unit of diversion will be a cookie which matches the unit of analysis. Therefore the analytic variance will be an overestimate compared to the empirical variance for Gross Conversion.\n",
    "\n",
    "For Retention and Net Conversion, the unit of diversion wil be a User-ID which matches the unit of analysis for Retention but not for Net Conversion. Therefore the analytical variance will be an overestimate compared to the empirical variance for Retention, but it will be comparable to the empirical variance for Net conversion.\n",
    "\n",
    "If there is time, it may be worth doing an empirical estimate of the variance for Gross Conversion and Retention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing\n",
    "\n",
    "Since there are 3 evaluation metrics, it will be best to use the Bonferroni correction to analyse the p-values of the hypothesis tests.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha & = 0.05 \\\\\n",
    "\\Rightarrow \\alpha^{*} & = \\frac{\\alpha}{m} = \\frac{0.05}{3} \\\\\n",
    "\\alpha^{*} & = 0.0167\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The difference of two proportions $d$ will have standard error\n",
    "$$SE_{d} = \\sqrt{\\frac{p_{pooled}(1-p_{pooled})}{n_1} + \\frac{p_{pooled}{(1-p_{pooled}})}{n_2}}$$\n",
    "\n",
    "Since this sizing calculation is performed before the A/B test is run, we will use the the probabilities acquired from the baseline values as a substitute for $p_{pooled}$ to estimate $SE_{d}$. We will use this to calculate the most conservative estimate for the number of enrollees required **per group** to ensure $\\alpha^{*} = 0.0167$, $\\beta = 0.2$ for the practical significance size $d_{min}$ of the metric that requires the most enrollees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pageviews required: 4739878.787878788\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def get_z_star(alpha):\n",
    "    return -norm.ppf(alpha/2)\n",
    "\n",
    "def get_beta(z_star, s, d_min, N):\n",
    "    SE = s / np.sqrt(N)  # The SE of d varies with N which subsequently affects the power \n",
    "    return norm.cdf(z_star*SE, loc=d_min, scale=SE)\n",
    "\n",
    "def required_size(s, d_min, N_max=100000, alpha=0.05, beta=0.2):\n",
    "    \"\"\"\n",
    "    s is the SE of the metric with N=1 in each group \n",
    "    \"\"\"\n",
    "    Ns = list(range(1,N_max))\n",
    "    for n in Ns:\n",
    "        if get_beta(get_z_star(alpha), s, d_min, n) <= beta:\n",
    "            return n\n",
    "    return -1\n",
    "\n",
    "num_enrollees = required_size(s=np.sqrt(0.53*(1-0.53)*2), d_min=0.01, alpha=0.05)\n",
    "num_pageviews = (num_enrollees/(0.08 * 0.20625))*2\n",
    "print(\"Number of pageviews required: {}\".format(num_pageviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27172"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_size(s=np.sqrt(0.1093125*(1-0.1093125)*2), d_min=0.0075, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685325.0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(27413/(0.08))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.26625"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "685325/20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
