{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AB Testing Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general methodology used to test out a new product or feature\n",
    "\n",
    "- Take two sets of users\n",
    "- One set is shown an existing product\n",
    "- Second set is given a treated version\n",
    "- How do the customers respond differently? determine which one is better based on some metric\n",
    "\n",
    "Can you use AB tests for everything?\n",
    "\n",
    "\"AB testing is good for optimizing an existing product but not good for developing a new product based on an existing one\"\n",
    "\n",
    "Amazon did AB tests for personalized recommendations and found that they had an increase in revenue when given the personalized recommendations.\n",
    "\n",
    "Google tested 41 different shades of blue\n",
    "\n",
    "LinkedIn tested a ranking process where they checked whether it's better to show news articles or an encouragement to add more contacts on a users \"stream\". (Use click-through rate as metric?)\n",
    "\n",
    "Amazon determined that ever 100ms increase in page load time decreased sales by 1%\n",
    "\n",
    "Need a consistent response from your control and experiment groups\n",
    "\n",
    "### What can't you do with AB tests?\n",
    "\n",
    "- Test out new experiences\n",
    "    - Change aversion - Users refuse to participate in the test\n",
    "    - Novelty effect - Too drastic of a change leads customers to \"try out everything\". Can't test out a specific treatment\n",
    "- No baseline for comparison\n",
    "    - Can't set up a control group if there is no baseline. \n",
    "- How much time do you need to have your users adapt to the new experience?\n",
    "    - Need the plateaued experience to make a \"robust decision\". The metric being observed will be noisy in the beginning and only when it has stabilised can you check for any statistically significant changes to your metric.\n",
    "- Long term effects are difficult to test\n",
    "    - Difficult to measure changes in your metric over a long time period where other aspects of your product or users will change (can't attribute the change in your metric to the treatment)\n",
    "- Can't test whether your missing something in your product\n",
    "    - No baseline, can;t set up a control and treatment group because what do you change about the treatment group?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other techniques\n",
    "\n",
    "- Logs of what users did on your website. Analyse them retrospectively or observationally to see if a hypothesis can be developed about what caused changes in their behaviour. This can then be used to design an experiment. \n",
    "- User experience research, focus groups, surveys, human evaluation\n",
    "- A/B testing gives quantitative data, other techniques give qualitative data.\n",
    "- A completetly new product is difficult test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In online A/B tests, you don't know much about your users. You're using online user data and so it's difficult to distinguish whether a user is a single person, internet cafe etc.\n",
    "\n",
    "The goal is to determine whether a new feature is desirable. To do this, you need to design an experiment that can be **repeatable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Case Study\n",
    "\n",
    "Audacity\n",
    "\n",
    "Creates online finance courses\n",
    "\n",
    "**User flow/Customer funnel**\n",
    "\n",
    "- Homepage visits\n",
    "- Explore the site\n",
    "- Create an account\n",
    "- Completion\n",
    "\n",
    "Listed in decreasing number of users.\n",
    "\n",
    "**The hypothesis:**\n",
    "\n",
    "Changing the \"Start Now\" button from orange to pink will **increase** how many students explore Audacity's courses\n",
    "\n",
    "**Possible metrics**\n",
    "\n",
    "- ~~**Total number of courses completed**~~\n",
    "    - Will take too much time. Students make take months to complete a course\n",
    "- ~~**How many users click on the \"Start Now\" button**~~\n",
    "    - Assumes that users who progress through the top of the customer funnel will eventually lead to more users being passed through the rest of the customer funnel\n",
    "    - In unequal control/treatment groups, the number of users in the group will affect the total number of clicks\n",
    "- **CTR: $\\frac{\\text{Number of clicks}}{\\text{Number of page views}}$**\n",
    "    - Called Click-Through-Rate\n",
    "    - Single users can click more than once and inflate the CTR\n",
    "- **CTP: $\\frac{\\text{Unique visitors who click}}{\\text{Unique vistors to the page}}$**\n",
    "    - Called Click-Through-Probability\n",
    "    - The better metric to use in this case.\n",
    "    \n",
    "**Updated metric**\n",
    "\n",
    "Changing the \"Start Now\" button from orange to pink will **increase** the Click-Through-Probability of the button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When do you use CTR vs. CTP?**\n",
    "\n",
    "Generally:\n",
    "\n",
    "- Use a rate when you want to measure **usability**\n",
    "    - Users have a number of different places they can press, you use a rate to measure how often the users clicked a specific button\n",
    "    - Will have to change the website to log every page view of the website and every click of a button\n",
    "- Use a probability when you want to measure **total impact**\n",
    "    - You don't want to count when users double clicked, reloaded etc when measuring a total effect (e.g. getting to the second level of a page)\n",
    "    - Will have to change the website to match each page view with all of their \"child clicks\" to count at most one click per page view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which distribution?**\n",
    "\n",
    "When producing the CTP, the sample proportion $p_0$ was computed to be 0.1 $(\\frac{100}{1000})$. When using a different sample to compute the CTP, the sample proportion was instead computed to be 0.15.\n",
    "\n",
    "Is 0.15 or 15% considered to be surprising? How do you know how variable your estimate is likely to be?\n",
    "\n",
    "We compare the sample proportion computed to the **binomial** distribution where we model each click as a bernoulli trial. Each unique visitor either clicks the button (success) or doesn't (failure).\n",
    "\n",
    "**Variance**\n",
    "\n",
    "We can use the standard error formula for a sample proportion $SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$ to estimate how variable the sample proportion $p_0$ should be as a result of sampling variability. Then we compare the second computed sample proportion (0.15) to the first relative to the variance to see if it is a surprising value or not. \n",
    "\n",
    "Either compute a CI or perform a hypothesis test\n",
    "\n",
    "<img src=\"images/difference_of_two_proportions.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical significance**\n",
    "\n",
    "We have to decide how big of a change is practically significant (aka substantive) to warrant changing the existing system. Statistical significance of any arbitrary difference in proportion can be achieved with a big enough sample size, however a small difference may not be practically significant. \n",
    "\n",
    "- Each change may require an investment in resources and so a small change may not warrant the investment\n",
    "- Online A/B tests have a smaller margin for practical significance\n",
    "- We need to make sure for online A/B tests that the change is **repeatable**. - We want a big enough sample size to have it so that the statistical significance bar is lower than the practical significance bar to ensure repeatability\n",
    "\n",
    "We will decide that a 2% change in CTP is practically significant\n",
    "\n",
    "**Size vs. Power tradeoff**\n",
    "\n",
    "The power of a hypothesis test is the probability that the test rejects the null hypothesis  $H_0$  when a specific alternative hypothesis  $H_1$  is true. The idea is that, given a practically significant effect size and a significance level, we want the hypothesis test to be able detect the effect (by rejecting the null hypothesis) at a high enough probability, which can be controlled by increasing the sample size.\n",
    "\n",
    "$\\alpha$ is the probability of making a type 1 error i.e. False Positive (reject the null hypothesis when it is True). Statistical power is equal to $1-\\beta$\n",
    "\n",
    "$\\beta$ is the probability of making a type 2 error (failing to reject the null hypothesis when it is false). \n",
    "\n",
    "Statistical power is equal to $1-\\beta$\n",
    "\n",
    "When the CI captures the null hypothesis $H_0$, the test is statistically insignificant (recall that you create a CI around the point estimate $\\hat{p}$ or $\\hat{d} = \\hat{p}_1 - \\hat{p}_2$). When the CI is outside of both $H_0$ **and** the practical significance level $d_{\\text{min}}$, then we can agree to launch the change. For cases inbetween where the CI is too wide or does not capture $H_0$ but does capture $d_{\\text{min}}$, we have to use our best judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ethics\n",
    "\n",
    "Participants in any kind of experimental test need to be adequately protected\n",
    "\n",
    "**Example**\n",
    "\n",
    "Facebook experiment to gauge the effect of altering user's news feeds on emotions. In particular for this study, there is no discussion by the experimenters on the benefits of the study being conducted.\n",
    "\n",
    "Four main principles govern the ethics of experiments with regard to the safety of their participants:\n",
    "\n",
    "1. Risk - What risks are the participants being exposed to?\n",
    "2. Benefit - What benefits might be the outcome of the study?\n",
    "3. Choice - What other choices do participants have?\n",
    "4. Privacy - What expectation and confidentiality do participants have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risk**\n",
    "\n",
    "Does the risk exceed that of \"minimal risk\". **Minimal risk** is defined as the probability (and magnitude) of harm that a participant would be exposed to in normal daily life. In most online A/B testing, the risk (of the test) does not exceed minimal risk although there are grey areas such as in the Facebook example.\n",
    "\n",
    "**Benefits**\n",
    "\n",
    "How might the results of the experiment help? It is important to be able to state what the benefit would be from completing the study. In most online A/B testing, the befits are around improving the product.\n",
    "\n",
    "**Alternatives**\n",
    "\n",
    "Do participants of the test really have a choice in whether to participate or not (and how does that effect the risks and the benefits)?. For example, in medical clinical trials testing our new drugs for terminal illnesses, the alternative for most participants is death. Thus, the risk allowable for participants, given **informed consent**, is quite high.\n",
    "\n",
    "For online experiments, if users do not want to participate in the testing, we must consider how this may inconvenience the users (for example costs, time, information etc. required to switch services).\n",
    "\n",
    "**Data Sensitivity**\n",
    "\n",
    "How sensitive is the data? What is the re-identification risk of individuals from the data? As the sensitivity and the risk increases, then the level of data protection must increase: confidentiality, access control, security, monitoring & auditing, etc. Sensitive data includes bank information, health information etc. whilst the re-identification risk of the data is determined by whether it is considered to be identified, pseudonymous, anonymous or anonymized.\n",
    "\n",
    "**Identified data** means that data is stored and collected with personally identifiable information. This can be names, IDs such as a social security number or driver’s license ID, phone numbers, etc. HIPAA is a common standard, and that standard has 18 identifiers (see the Safe Harbor method) that it considers personally identifiable. Device id, such as a smartphone’s device id, are considered personally identifiable in many instances.\n",
    "\n",
    "**Anonymous data** means that data is stored and collected without any personally identifiable information. This data can be considered **pseudonymous** if it is stored with a randomly generated id such as a cookie that gets assigned on some event, such as the first time that a user goes to an app or website and does not have such an id stored.\n",
    "\n",
    "In most cases, anonymous data still has time-stamps -- which is one of the HIPAA 18 identifiers. Why? Well, we need to distinguish between anonymous data and anonymized data. **Anonymized data** is identified or anonymous data that has been looked at and guaranteed in some way that the re-identification risk is low to non-existent, i.e. given the data, it would be hard to impossible for someone to be able to figure out which individual this data refers to. Often times, this guarantee is done statistically, and looks at how many individuals would fall into every possible bucket (i.e., combination of values).\n",
    "\n",
    "What this means is that anonymous data may still have high re-identification risk. **Aggregated data** is usually not sensitive\n",
    "\n",
    "For online A/B testing, questions that must be considered include:\n",
    "\n",
    "- Are users being informed about the data being gathered via a ToS or Privacy policy?\n",
    "- What user identifiers are tied to the data being gathered? are there any identified data being gathered?\n",
    "- What type of data is being collected? Any health or financial data?\n",
    "- What level of confidentiality and security is the data subject to? Is the access of data being logged and audited.\n",
    "\n",
    "**Informed consent**\n",
    "\n",
    "Participants are told about the risks that they may face if they take part in the study, what benefits might result, what other options they have, what data is being gathered and how that data is being handled. Typically informed consent is handled by giving participants a document detailing all of the aforementioned information and participants can then choose whether they want to participate or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "There are two main uses for metrics in A/B testing:\n",
    "\n",
    "- Invariant checking - The metrics that shouldn't change across your treatment and control. \n",
    "- Evaluation - To check whether the treatment group is \"performing better\" than your evaluation group\n",
    "\n",
    "Two types of Evaluation metrics\n",
    "\n",
    "- High level metrics\n",
    "- Well defined metric \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level metrics\n",
    "\n",
    "These usually relate to the business objective. They are not directly used to perform the A/B test but they help to decide on which metrics will eventually be used to do so.\n",
    "\n",
    "In a customer driven business, we can use a **customer funnel** to brainstorm the possible high level metrics that will be important.\n",
    "\n",
    "<img src=\"\">\n",
    "\n",
    "Each level in the customer funnel is a high level metric that can be used to answer questions such as \"what business objective are you tracking?\" \n",
    "\n",
    "You can categorize every metric into three different types:\n",
    "- Count\n",
    "- Rate\n",
    "- Probability\n",
    "\n",
    "You want the metric that you eventually use in an A/B test to be a KPI for your business, but these high level metrics still need to be transformed into formal definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficult metrics\n",
    "\n",
    "Some metrics will be difficult to use in an A/B test. This comes down to two specific reasons\n",
    "\n",
    "- **Time**: For some metrics, the data will just take too long to collect. It is also difficult to ensure the smooth operation of a long-term ongoing online experiment  \n",
    "- **Availability**: For other metrics, the data may not be readily available to use. The business may not have access to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other techniques for coming up with metrics\n",
    "\n",
    "There are other techniques you can use to help you get an understanding of your users, which you can use to come up with ideas for metrics, validate your existing metrics, or even brainstorm ideas of what you might want to test in\n",
    "your experiments in the first place. See [here](https://s3-us-west-2.amazonaws.com/gae-supplemental-media/additional-techniquespdf/additional_techniques.pdf)\n",
    "\n",
    "<img src=\"images/gathering_additional_data.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well defined metrics\n",
    "\n",
    "To convert a high level metric into a well defined metric, you must consider two mains things:\n",
    "\n",
    "- What data are we actually going to look at to compute the metric (cookies? page views? clicks? are we going to filter the data?)\n",
    "- Given the events (e.g. clicks) how will we summarize the metric? (mean? median? etc)\n",
    "\n",
    "Essentially you must now consider the logistics of collecting and summarizing the metric\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\text{CTP} = \\frac{\\text{Unique visitors who click}}{\\text{Unique vistors to the page}}$\n",
    "\n",
    "This is not a well defined metric yet\n",
    "\n",
    "- how do we determine when two events are from the same user? \n",
    "    - Let's use a cookie\n",
    "- What time period do we use to count events? hour? day? week?\n",
    "\n",
    "\n",
    "A few well defined metrics that act like CTP are:\n",
    "\n",
    "1 - $\\text{Cookie Probability} = \\frac{\\text{Number of cookies that click}}{\\text{Total number of cookies}}$ for every hour (or any other time interval)\n",
    "\n",
    "2 - $\\text{Pageview probability} = \\frac{\\text{Pageview with click}}{\\text{Total number of page views}}$ for every hour (or any other time interval)\n",
    "\n",
    "3 - $\\text{CTR (with time period)} = \\frac{\\text{Total number of clicks}}{\\text{Total number of page views}}$ for every hour (or any other time interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting and filtering data\n",
    "\n",
    "Often times you want to filter out any **unexpected** traffic or participants from your experiments. An example would be spam IP addresses that don't represent a typical user (and thus you do not care how a change to your website affects these participants and thus do not want these participants to skew your metric). \n",
    "\n",
    "Filtering out changes that are targeted towards a specific subset of users is also important because you want to avoid **diluting your results**. Results are diluted when a change in your metric can no longer be detected due a **reduction in power** of the A/B test where you have included participants in your experiment whom you know will not be affected. $N$ is then much larger than it should be, and legitimate changes to the behaviour of the participants (that you suspect should be affected) may be considered as **sampling variation** in the hypothesis test.\n",
    "\n",
    "Filtering is used to **de-bias** the data whilst avoiding **introducing** bias to the data. Bias may be introduced if you choose to use a filter but it is removing participants from certain subsets disproportionately. The experiment is no longer **randomized**.\n",
    "\n",
    "Use **slicing** to check if your filter is introducing bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary metrics\n",
    "\n",
    "For count metrics in particular and other well defined metrics (e.g. load time of a video) you have many different ways to summarize the metric e.g. mean, median, 50%ile, 75%ile, 90%ile etc. \n",
    "\n",
    "To choose between these metrics, you should consider two main factors:\n",
    "\n",
    "- **Sensitivity** - You want your metric to be sensitive enough to changes in the treatment\n",
    "- **Robustness** - You want your metric to be robust to ongoing changes in your system that aren't the treatment but you have no control over\n",
    "\n",
    "To choose how to summarize the metric, you can perform a **retrospective analysis** and produce a **histogram** of the metric. For skewed distributions, the mean is not necessarily the best measure of central tendency and a median or %ile may be more appropriate.\n",
    "\n",
    ">Let’s talk about some common distributions that come up when you look at real user data. For example, let’s measure the rate at which users click on a result on our search page, analogously, we could measure the average staytime on the results page before traveling to a result. In this case, you’d probably see what we call a **Poisson distribution**, or that the stay times would be exponentially distributed. Another common distribution of user data is a “power-law,” Zipfian or **Pareto distribution**. That basically means that the probability of a more extreme value, z, decreases like 1/z (or 1/z^exponent). This distribution also comes up in other rare events such as the frequency of words in a text (the most common word is really really common compared to the next word on the list). These types of heavy-tailed distributions are common in internet data. Finally, you may have data that is a composition of different distributions - latency often has this characteristic because users on fast internet connection form one group and users on dial-up or cell phone networks form another. Even on mobile phones you may have differences between carriers, or newer cell phones vs. older text-based displays. This forms what is called a **mixture distribution** that can be hard to detect or characterize well. The key here is not to necessarily come up with a distribution to match if the answer isn’t clear - that can be helpful - but to choose summary statistics that make the most sense for what you do have. If you have a distribution that is lopsided with a very long tail, choosing the mean probably doesn’t work for you very well - and in the case of something like the Pareto, the mean may be infinite!\n",
    "\n",
    "There are 4 main types of summary metrics\n",
    "\n",
    "<img src=\"images/categories_for_summary_metrics.png\" width=600>\n",
    "\n",
    "A review of the literature may need to be considered to select the best summary metric (e.g. say a study showed that people take 5 seconds to internalize the information of a web page, then use the number of users who spent 5 seconds or longer on a page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity or Robustness\n",
    "\n",
    "The mean is **not a robust metric** as it is sensitive to outliers, the mean of a metric is heavily influnced by values that proportional to their size. The median is robust but may not be sensitive enough.\n",
    "\n",
    "To measure the **sensitivity** of a summary metric, you can perform a **retrospective study** and look at previous changes to the website (that match the change you're trying to test) to see if it affected the metric in ways that you expect. You can also perform separate simple experiments to check if your summary metric responds to the change in expected ways. Looking back at previous experiments performed may also be done.\n",
    "\n",
    "To measure the **robustness** of a summary metric, you can perform an **A/A test** where you don't change anything and see if the metric picks up on any spurious changes. The metric is not robust if there are more statistically significant changes to your metric between A/A groups than expected due to sampling variation. (See the spreadsheet for example). You can also perform retrospective studies or simple experiments and check to see if your metric is robust between comparable groups.\n",
    "\n",
    "Here is an example of a simple experiment to check for robustness where each video should be comparable in latency (each video is the same size, resolution etc). \n",
    "\n",
    "<img src=\"images/robustness.png\" width=600>\n",
    "\n",
    "\n",
    "Here is an example of a simple experiment to check for sensitivity where the higher the video number, the lower the resolution (i.e. we expect the latency to decrease w.r.t video number)\n",
    "\n",
    "<img src=\"images/sensitivity.png\" width=600>\n",
    "\n",
    "**From the experiments, the 85%ile might be a good metric to use based on sensitivity and robustness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute vs Relative difference\n",
    "\n",
    "How do you compute the difference in your metric between treatment and control? If you run multiple experiments, then using a relative difference (e.g. percentage change) means that you only need to define one practical effect size (e.g. 2% increase).\n",
    "\n",
    "Absolute vs. relative difference\n",
    "\n",
    "Suppose you run an experiment where you measure the number of visits to your homepage, and you measure 5000 visits in the control and 7000 in the experiment. Then the absolute difference is the result of subtracting one from the other, that is, 2000. The relative difference is the absolute difference divided by the control metric, that is, 40%.\n",
    "\n",
    "Relative differences in probabilities\n",
    "\n",
    "For probability metrics, people often use percentage points to refer to absolute differences and percentages to refer to relative differences. For example, if your control click-through-probability were 5%, and your experiment click-through-probability were 7%, the absolute difference would be 2 percentage points, and the relative difference would be 40 percent. However, sometimes people will refer to the absolute difference as a 2 percent change, so if someone gives you a percentage, it's important to clarify whether they mean a relative or absolute difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variability\n",
    "\n",
    "You need to compute the variance of the metric in order to check whether a statistically (or practically) significant difference in your metric has resulted from the treatment.\n",
    "\n",
    "For some non-standard metrics (e.g. ratios, 90%ile) it is difficult to calculate the variance analytically, and so it is better to compute the variance empirically. Here is a list of common metrics and their sample distribution/sample variance\n",
    "\n",
    "<img src=\"images/metrics_and_variance.png\" width=600>\n",
    "\n",
    "For difficult metrics where you can't calculate the variance analytically, you may also not want to make any assumptions about the distribution of the metric. If so, you have a few options\n",
    "\n",
    "- **Sign test** - Run 20 experiments, if on 15 of those experiments the metric increased, use binomial distribution to perform a hypothesis test to check the chance of this happening due to sampling variation (assume p - the chance of success is 50% for no difference between the two groups).\n",
    "    - Doesn't help estimate the size of the effect (practical significance)\n",
    "    - Part of a broader range of methods called non-parametric methods\n",
    "\n",
    "- **Emperically compute confidence interval** - This can be done by performing multiple experiments (possibily on different sample sizes) and calculating the sample variance (or sample standard deviation) of the metric between the experiments and use that as an estimate of the population variance.\n",
    "\n",
    "Empirical CIs/Variances are done using A/A tests. See the spreadsheet for an example of this. \n",
    "\n",
    "To compute empirical CIs/Variances, you can either run many experiments or **bootstrap** from one large experiment to compute the sample standard deviation (which is an unbiased estimate of the standard error of the metric). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing an Experiment\n",
    "\n",
    "### Units of diversion\n",
    "\n",
    "In A/B testing, we need to assign different subjects to our control and treatment groups. How do you decide what is a subject in the experiment? With a user visible change, ideally we want a unique person to be a subject in our experiment, however there only exists imperfect proxys for a unique people in online testing. The way we define/approximate a subject in our experiment is called the **unit of diversion** where you divert each unit into the control or treatment groups.\n",
    "\n",
    "We may try to distinguish users by\n",
    "\n",
    "- **User ID**\n",
    "    - A single user may sometimes have multiple accounts\n",
    "    - is personally identifiable\n",
    "- **Cookie** \n",
    "    - If you switch browser, you get assigned a different cookie\n",
    "    - If you clear your history, you get assigned a different cookie\n",
    "- **Event**\n",
    "    - Users will not get a consistent experience\n",
    "- **Device ID**\n",
    "    - Only available for mobile\n",
    "    - tied to a specific device\n",
    "    - is personally identifiable\n",
    "- **IP address**\n",
    "    - Can change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency of diversion\n",
    "\n",
    "There are three main things to consider when selecting the unit of diversion\n",
    "\n",
    "**Consistency and Statefulness**\n",
    "\n",
    "The idea is to make sure that the subjects in your experiment has a consistent experience during testing i.e. the experience of each subject is the same throughout the lifetime of the experiment.\n",
    "\n",
    "For user visible changes, it's best to use User ID or Cookies to ensure consistency of diversion\n",
    "\n",
    "For non user visible changes e.g. latency changes, backend infrastructure changes, ranking changes etc, you do not need to worry about consistency and thus you can used an event based unit of diversion\n",
    "\n",
    "\n",
    "If the metric you're measuring is some kind of learning effect i.e. if a user adapts to changes made, then you need to track the same user over a long period of time. You then need to use a **stateful** unit of diversion that is attached to a single user for which User ID and Cookie is most appropriate.\n",
    "\n",
    "**Ethical considerations**\n",
    "\n",
    "Cookie and User ID are personally identifiable units of diversion. To use these then, you need to consider whether your prepared to go through all of the necessary data protection steps that are required when collecting such data. This includes for example collecting informed consent from your participants.\n",
    "\n",
    "**Variability**\n",
    "\n",
    "When the **unit of analysis** is the same as the unit of diversion, you have a lower **empirical variability** in your metric due to sampling variation (which is desirable as it increases the power of the A/B test). This means that the analytical estimate of the variance of your metric is an overestimate. The unit of analysis is the denominator of the metric (if your metric is a ratio).\n",
    "\n",
    "You then want to choose a **unit of diversion** that matches the denominator of your metric when you can (which usually ends up being an event based diversion).\n",
    "\n",
    "<img src=\"images/unit_of_diversion_unit_of_analysis.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter vs. Intra user experiment\n",
    "\n",
    "A/B testing is an inter user experiment where there are different participants in each of the control and treatment groups. More information in [this paper](http://www.cs.cornell.edu/people/tj/publications/chapelle_etal_12a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target population\n",
    "\n",
    "There are various reasons why you would want to target a specfic group in the population. This is particularly important if you know in advance who will be affected by the changes made in your treatment group. Some reasons to target a specific group in your population include (but is not limited to):\n",
    "\n",
    "- If you're testing a high-profile feature that you're unsure about whether it will be released or not, you may want to restrict the change so that only a limited amount of users experience it and as a result avoid getting press coverage etc.\n",
    "- You may want to restrict the target population by language as you want to avoid going through the trouble of testing in different languages.\n",
    "- You may not be sure whether your feature works on all browsers and thus you may want to restrict the target population to specific browsers\n",
    "- If you're running multiple experiments at your company, you may not want to overlap participants of the study (have subjects take part in multiple experiments)\n",
    "- You may not want to dilute the effect of your experiment if you know your change will only affect a specific group of your population. Increasing $N$ whilst keeping the number of participants affected the same will reduce the effect size e.g. the difference between proportions for CTP and thus reduce the power of your experiment (see filtering section)\n",
    "\n",
    "You need to ask the engineering team if they have an idea about who the changes will target.\n",
    "\n",
    "After the experiment, you will want to test the changes on your global population just to check if there are any unwanted effects on the traffic you were not targetting. An example of diluting the effect size is shown below:\n",
    "\n",
    "<img src=\"images/diluting_variance_1.png\" width=600>\n",
    "\n",
    "<img src=\"images/diluting_variance_2.png\" width=600>\n",
    "\n",
    "Adding all of the unaffected traffic outside of the population that would be affected (New Zealand traffic) diluted the difference of the poportions (disproportionately to the reduction in SE) and thus the difference is no longer statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohort\n",
    "\n",
    "If you just divert your users by Cookie or User ID, you may have participants drop out of your groups or join your groups during the lifetime of your experiment. Cohorts are groups of users who entered the experiment at the same time and/or have a similar level of activity. You typically want to use a cohort in experiments when:\n",
    "\n",
    "- You're looking for learning effects (whether users are adapting to a change or not)\n",
    "- Examining user retention\n",
    "- Want to increase user activity\n",
    "- Anything requiring the user to be established i.e. has used the site for at least a specific number of hours\n",
    "\n",
    "This is essentially a form of filtering to ensure that the participants of the study are who you want to test the change on. In the audacity case study, they may change the structure of a specific course and only target a cohort of participants who haven't completed the course yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing the experiment\n",
    "\n",
    "Before we only considered how to size the experiment based on practical significance, statistical significance and sensitivity (e.g. ensuring the experiment has at least 60% power). From what we just learned, we now also need to take into account the unit of diversion vs. unit of analysis as well as the target population and then decide whether the size is realistic relative to how long you have to run the experiment.\n",
    "\n",
    "To perform the calculation, you just do a power calculation for $N$, the sample size required to ensure a certain $\\alpha$ (usually 0.05), $\\beta$ (usually 0.2) and $d_{\\text{min}}$ whilst estimating SE empirically (or analytically for simple metrics).\n",
    "\n",
    "To reduce the size of the experiment required whilst maintaining the same $\\alpha$, $\\beta$ and $d_{\\text{min}}$, you could try:\n",
    "- Changing the unit of diversion to match the unit of analysis\n",
    "- Target experiment to specific traffic (filtering or using cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing triggering\n",
    "\n",
    "Even after deciding on a well-defined metric, you often don't know beforehand whether the population that is supposed to be exposed to the changes in your treatment are truly exposed to the changes or not. A good idea is to run a \"pilot\" where you turn on the changes and observe who actually are affected and see if it matches your intutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration\n",
    "\n",
    "Depending on the sizing calculations relative to the traffic on your website, you will need to decide on a suitable **duration** and **exposure proportion**.\n",
    "\n",
    "For example, if your website gets $100000$ users per day, and the sizing calculations resulted in a required sample size of $N=1000000$, if you expose your change to your website's entire traffic, then you will need to run your experiment for 10 days.\n",
    "\n",
    "However, exposing the change to the entire traffic isn't a good idea because of reasons mentioned before in the \"Target Population\" section such as avoiding press coverage, overlapping experiments, filtering by cohorts etc.\n",
    "\n",
    "Reducing the exposure proportion will then increase the required duration of your experiment.\n",
    "\n",
    "Another reason is the fact that you are trying to account for confounding factors by **randomizing across your unit of diversion**. However, other **temporal factors** might be affecting your subjects/metric which you have to also account for. For example, if you run your online experiment over a holiday, the results of the experiment (if statistically and practically significant) may not apply all year round. \n",
    "\n",
    "By having a longer running experiment, or by running your experiment on specific days of the week/year, you can ensure that the results of your experiment are **generalizable**.\n",
    "\n",
    "<img src=\"images/limit_exposure.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning effects\n",
    "\n",
    "When you want to measure learning effects, you're trying to quantify whether users are adapting to the changes (in your treatment group) or not. Measuring this type of effect is difficult due to the two reasons mentioned before:\n",
    "\n",
    "**Change aversion** - Users refuse to participate in the test\n",
    "**Novelty effect** - Too drastic of a change leads customers to \"try out everything\". Can't test out a specific treatment\n",
    "\n",
    "The key idea is that for learning effects, you need to consider the duration of the experiment that is required for the user behaviour to plateau or converge.\n",
    "\n",
    "These are the things you need to keep in mind when measuring learning effects (some of which has been mentioned already)\n",
    "\n",
    "- You need a **stateful** unit of diversion\n",
    "- You need to consider the dosage (how often the user experiences the change) and thus you should select a **cohort** based on dosage\n",
    "- Learning effects are usually high risk and thus you'll probably want to test on a smaller proportion of users for a longer duration of time (to meet the sample size $N$ quota)\n",
    "\n",
    "**pre-period vs. post-period experiments** is a method you can use to ensure that user experience and populations are comparable between your control and treatment groups. The idea is that you run A/A tests **before and after** the actual A/B test on your treatment and control groups to ensure that there are no inherent differences between the two groups. This ensures that after your treatment group is exposed to the change, the differences between the two groups can be attributed to the treatment and not any other confounding factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
